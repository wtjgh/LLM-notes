# LLM 
| Benchmark   | Focus                              | Domain                   | Evaluation Criteria                           |
|-------------|------------------------------------|--------------------------|-----------------------------------------------|
| SOCKET [paper]      | 社交知识                        | 具体下游任务 | 社交语言理解           |
| MME [paper]      | 多模态LLMs                        | 多模态任务 | 感知和认知能力           |
| Xiezhi [paper][GitHub]      | 综合领域知识   | 通用语言任务 | 跨多个基准测试的整体性能   |
| Choice-75 [paper][GitHub]  | 脚本学习 | 具体下游任务 | LLMs的整体性能 |
| CUAD [paper] | 法律合同审查 | 具体下游任务 | 法律合同理解 |
| TRUSTGPT [paper] | 伦理 | 具体下游任务 | 毒性、偏见和价值对齐 |
| MMLU [paper]      | 文本模型                        | 通用语言任务 | 多任务准确性           |
| MATH [paper] | 数学问题  | 具体下游任务 | 数学能力 |
| APPS [paper]|编码挑战能力 | 具体下游任务 | 代码生成能力|
| CELLO[paper][GitHub] |复杂指令 | 具体下游任务 | 计数限制、答案格式、任务规定的短语和输入相关查询|
| C-Eval [paper][GitHub | 代码生成 | 具体下游任务 | 生成代码质量、多样性和速度 |
以上是表格的中文翻译，保留了原始的格式和内容。
