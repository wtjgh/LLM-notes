# LLM 评估标准
LLM的评价指标大致可以分为两大类别：自动评价，人工评价。自动评价方法基于计算机算法和可量化的指标，能够快速且高效地评估LLM模型的性能。而人工评价则侧重于人类专家的主观判断和质量评估，能够提供更深入、细致的分析和意见。由
于人工评价成本高且不易于实施，这里我们仅介绍自动评价相关内容。关于自动评价，也会从基础评测任务，高级能力评估，公开基准以及指令跟随这四个方面介绍。
## 基础评测任务
基础评测任务即为评测自然语言处理中的基础任务。共三大类，分别为：语言生成，知识利用与复杂推理。
### 语言生成
根据任务定义，现有语言生成的任务主要可以分为语言建模、条件文本生成和代码合成任务。需要注意的是，代码合成不是典型的自然语言处理任务，但可以直接地用（经过代码数据训练的）LLM 以类似自然语言文本生成的方法解决，因此也纳入
讨论范围。
#### 语言建模： 
语言建模是 LLM 的基本能力，旨在基于前一个token 预测下一个 token，主要关注基本的语言理解和生成能力。典型的语言建模数据集包括 Penn Treebank 、WikiText-103和 Pile，其中 <font color='red'>困惑度（perplexity）
指标</font> 通常用于评估零样本情况下模型的性能。相关研究表明， LLM 在这些评估数据集上相较于之前效果最好的方法带来了实质性的性能提升。为了更好地测试文本的长程依赖的建模能力， LAMBADA 数据集要求 LLM 基于一段上下文来预测句子的最后一个单词。然后使用预测的最后一个单词的准确性和困惑度来评估 LLM 性能。
#### 条件文本生成： 
作为语言生成中的一个重要话题，条件文本生成旨在基于给定的条件生成满足特定任务需求的文本，通常包括机器翻译、文本摘要和问答系统 等。为了衡量生成文本的质量，通常使用自动化指标（如准确率、 BLEU 和 ROUGE）和人类评分来评估性能。人类评估本文暂且不讨论。由于 LLM 具有强大的语言生成能力，它们在现有的数据集上取得了显著的性能，甚至超过了人类（在测试数据集上的）表现。例如，当仅给出 32 个示例作为输入时， GPT-3 通过 ICL （上下文学习）能够在 SuperGLUE 的平均得分上超过使用完整数据微调的 BERT-Large；在 MMLU 指标上，一个 5-样本的Chinchilla的准确率几乎是人类平均准确率的两倍；而在5-样本的设定下， GPT-4取得了当前最优秀的性能，平均准确率比之前的最佳模型提高了超过 10%。于是，人们开始关注现有的条件文本生成任务，能否很好地评估和反映 LLM的能力。考虑到这个问题，研究人员试图通过收集目前无法解决的任务（即 LLM 无法取得良好表现的任务）或创建更具挑战性的任务（例如超长文本生成）来制定新的评估基准，
例如 BIG-bench Hard。此外，最近的研究还发现自动化指标可能会低估 LLM 的生成质量。在 OpenDialKG 中， ChatGPT 在 BLEU 和 ROUGE-L 指标上表现不如微调的 GPT-2，但在人类评分中获得了更多的好评。因此，
