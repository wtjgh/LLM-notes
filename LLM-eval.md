# LLM 评估标准
LLM的评价指标大致可以分为两大类别：自动评价，人工评价。自动评价方法基于计算机算法和可量化的指标，能够快速且高效地评估LLM模型的性能。而人工评价则侧重于人类专家的主观判断和质量评估，能够提供更深入、细致的分析和意见。由
于人工评价成本高且不易于实施，这里我们仅介绍自动评价相关内容。关于自动评价，也会从基础评测任务，高级能力评估，公开基准以及指令跟随这四个方面介绍。
## 基础评测任务
基础评测任务即为评测自然语言处理中的基础任务。共三大类，分别为：语言生成，知识利用与复杂推理。
### 语言生成
根据任务定义，现有语言生成的任务主要可以分为语言建模、条件文本生成和代码合成任务。需要注意的是，代码合成不是典型的自然语言处理任务，但可以直接地用（经过代码数据训练的）LLM 以类似自然语言文本生成的方法解决，因此也纳入
讨论范围。
#### 语言建模： 
语言建模是 LLM 的基本能力，旨在基于前一个token 预测下一个 token，主要关注基本的语言理解和生成能力。典型的语言建模数据集包括 Penn Treebank 、WikiText-103和 Pile，其中困惑度（perplexity）指标通常用于评估零样本情况下模型的性能。相关研究表明， LLM 在这些评估数据集上相较于之前效果最好的方法带来了实质性的性能提升。为了更好地测试文本的长程依赖的建模能力， LAMBADA 数据集要求 LLM 基于一段上下文来预测句子的最后一个单词。然后使用预测的最后一个单词的准确性和困惑度来评估 LLM 性能。量化评价指标：困惑度（perplexity）。
#### 条件文本生成： 
作为语言生成中的一个重要话题，条件文本生成旨在基于给定的条件生成满足特定任务需求的文本，通常包括机器翻译、文本摘要和问答系统 等。为了衡量生成文本的质量，通常使用自动化指标（如准确率、 BLEU 和 ROUGE）和人类评分来评估性能。人类评估本文暂且不讨论。由于 LLM 具有强大的语言生成能力，它们在现有的数据集上取得了显著的性能，甚至超过了人类（在测试数据集上的）表现。例如，当仅给出 32 个示例作为输入时， GPT-3 通过 ICL （上下文学习）能够在 SuperGLUE 的平均得分上超过使用完整数据微调的 BERT-Large；在 MMLU 指标上，一个 5-样本的Chinchilla的准确率几乎是人类平均准确率的两倍；而在5-样本的设定下， GPT-4取得了当前最优秀的性能，平均准确率比之前的最佳模型提高了超过 10%。于是，人们开始关注现有的条件文本生成任务，能否很好地评估和反映 LLM的能力。考虑到这个问题，研究人员试图通过收集目前无法解决的任务（即 LLM 无法取得良好表现的任务）或创建更具挑战性的任务（例如超长文本生成）来制定新的评估基准，
例如 BIG-bench Hard。此外，最近的研究还发现自动化指标可能会低估 LLM 的生成质量。在 OpenDialKG 中， ChatGPT 在 BLEU 和 ROUGE-L 指标上表现不如微调的 GPT-2，但在人类评分中获得了更多的好评。量化评价指标：准确率、 BLEU 和 ROUGE。
#### 代码生成：
除了生成高质量的自然语言外，现有的 LLM 还表现出强大的生成形式语言的能力，尤其是满足特定条件的计算机程序（即代码），这种能力被称为代码生成。与自然语言生成不同，由于生成的代码可以直接用相应的编译器或解释器执行，现有的工作主要通过计算测试用例的通过率（即 pass@k）来评估 LLM 生成的代码的质量。量化评价指标：pass@k。
### 知识利用
知识利用是一种智能系统基于事实证据的支撑，完成知识密集型任务的重要能力（例如常识问题回答和事实补全）。具体而言，它要求 LLM 适当地利用来自预训练语料库的丰富事实知识，或在必要的时候检索外部数据。特别地，问答和知识补全已经成为评估这一能力的两种常用任务。根据测试任务（问答或知识补全）和评估设定（有或没有外部资源），我们将现有的知识利用任务分为三种类型，即闭卷问答，开卷问答和知识补全。
#### 闭卷问答：
闭卷问答任务测试 LLM 从预训练语料库中习得的事实知识。 LLM 只能基于给定的上下文回答问题，而不能使用外部资源。为了评估这一能力，可以利用几个数据集，包括 Natural Questions、 Web Questions和TriviaQA。量化评价指标：准确率。
#### 开卷问答：
与闭卷问答不同，在开卷问答任务中， LLM 可以从外部知识库或文档集合中提取有用的证据，然后基于提取的证据回答问题。典型的开卷问答数据集（例如， NaturalQuestions、 OpenBookQA和 SQuAD）与闭卷问答数据集有所重叠，但是前者包含外部数据源，例如维基百科。在开卷问答任务中，量化评价指标：准确率和 F1-score。
### 知识补全
在知识补全任务中， LLM（在某种程度上）可以被视为一个知识库，补全或预测知识单元（例如知识三元组）的缺失部分。这种任务可以探索和评估 LLM 从预训练数据中学习到的知识的数量和种类。现有的知识补全任务可以粗略地分为知识图谱补全任务（例如 FB15k-237和WN18RR和事实补全任务（例如， WikiFact]），分别旨在补全知识图谱中的三元组和有关特定事实的句子。量化评价指标：MeanRank，Hits，MRR。






