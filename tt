def init_dataset(self):
        for data, lable in zip(self.datalist, self.labellist):
            tokenized_data = self.tokenizer.encode_plus(data,
                                                        truncation=True,
                                                        return_tensors='pt',
                                                        verbose=False)
            tokenized_lable = self.tokenizer.encode_plus(lable,
                                                        truncation=True,
                                                        return_tensors='pt',
                                                        verbose=False)
            self.dataset.append({
                'input_ids':
                tokenized_data.input_ids[0],
                'labels':
                tokenized_lable.input_ids[0],
                'attention_mask':
                tokenized_data.attention_mask[0]                
            })
